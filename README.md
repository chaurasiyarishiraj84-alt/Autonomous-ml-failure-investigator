---
title: Autonomous ML Failure Investigator
emoji: ðŸ§ 
colorFrom: purple
colorTo: blue
sdk: docker
app_port: 7860
pinned: true
license: mit
---

# ðŸ§  Autonomous ML Failure Investigator

> A Tier-1+ production-grade MLOps system that continuously monitors deployed ML models, detects failures, explains root causes, and recommends corrective actions â€” autonomously and intelligently.

> ðŸ’¡ **Running on HuggingFace Spaces?** No setup needed â€” the app is already live above. The installation and localhost instructions below are for **local development only.**

**Autonomous ML Failure Investigator** is an advanced Model Observability, Drift Detection, and Root Cause Analysis (RCA) system designed to monitor deployed machine learning models in real time, detect failures, explain why they occur, and provide actionable recommendations with human-in-the-loop governance.

This project simulates industry-grade MLOps monitoring platforms used in production environments by companies like Google, Netflix, and Amazon.

---

## ðŸ©º The Core Idea

Think of this system as a **doctor for ML models in production.**

| Doctor | This System |
|---|---|
| Patient | Deployed ML model |
| Symptoms | Accuracy drop, drift, anomalies |
| Tests | Metrics, KS-test, SHAP |
| Diagnosis | Root Cause Analysis |
| Explanation | Human-readable report |
| Medicine | Retrain, rollback, fix pipeline |
| Surgery (optional) | Auto-fix with human approval |
| Medical history | Failure memory & learning |

---

## ðŸš€ What This Project Does

For any deployed ML model, this system:

- **Continuously monitors** prediction behavior via API
- **Learns a baseline** of normal model performance automatically
- **Detects drift and anomalies** using statistical tests and ML
- **Explains why failures happen** through Root Cause Analysis
- **Identifies which features** are responsible for failures
- **Generates actionable recommendations** specific to each failure type
- **Requires human approval** before any critical action is taken
- **Produces a diagnostic report** for model improvement
- **Learns from past failures** to improve detection over time

---

## ðŸ—ï¸ System Architecture

```
User connects model via API URL
         â†“
Data Capture Layer          â†’ logs inputs, predictions, latency
         â†“
Baseline Learning           â†’ learns normal behavior automatically
         â†“
Drift & Anomaly Detection   â†’ KS-test, variance, feature shift
         â†“
Root Cause Analysis (RCA)   â†’ why did it fail? which feature?
         â†“
Explanation Layer           â†’ plain English findings
         â†“
Recommendation Engine       â†’ specific actionable fixes
         â†“
Human Approval (Governance) â†’ approve/reject with audit log
         â†“
Continuous Learning         â†’ improves from past failures
```

---

## âœ… Compatible API Requirements

Any API URL you connect to this system must follow **3 simple rules:**

### 1. Must be a public URL
```
âœ… https://anything.hf.space/predict
âœ… https://any-public-api.com/predict
âŒ http://127.0.0.1:9000/predict   â† localhost only works for local development
```

### 2. Must accept POST requests with a JSON body
```json
POST /predict
Content-Type: application/json

{"text": "anything"}
```

### 3. Must return JSON with a `confidence` field
```json
{
    "prediction": "any string",
    "confidence": 0.85
}
```

> `confidence` must be a **float between 0.0 and 1.0** â€” this is the most critical field.

---

### ðŸ§ª Ready-to-Use Test Models

Deploy these on HuggingFace to simulate different failure scenarios:

| Model | URL | Behavior |
|---|---|---|
| ðŸŸ¢ Healthy Model | `https://rishi-raj123-healthy-model.hf.space/predict` | confidence 0.80â€“0.95 â€” normal |
| ðŸ”´ Low Confidence | `https://rishi-raj123-low-confidence-model.hf.space/predict` | confidence 0.10â€“0.30 â€” triggers alerts |
| ðŸŸ¡ Random Model | `https://rishi-raj123-random-model.hf.space/predict` | confidence 0.0â€“1.0 â€” unpredictable |
| ðŸ“‰ Drift Model | `https://rishi-raj123-drift-model.hf.space/predict` | confidence degrades over time |

Each model is a minimal FastAPI app with a single `POST /predict` endpoint â€” fully compatible with all system features including drift detection, RCA, and recommendations.

---

## ðŸ“‚ Project Structure

```
autonomous-ml-failure-investigator/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                              # FastAPI entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ monitoring.py                # Monitoring API endpoints
â”‚   â”‚       â””â”€â”€ recommendation.py            # Recommendation routes
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ detection/
â”‚   â”‚   â”‚   â”œâ”€â”€ anomaly_detector.py          # Rule + ML anomaly detection
â”‚   â”‚   â”‚   â”œâ”€â”€ drift_detector.py            # KS-test, variance, feature drift
â”‚   â”‚   â”‚   â””â”€â”€ metric_checker.py            # Threshold-based metric checks
â”‚   â”‚   â”œâ”€â”€ probing/
â”‚   â”‚   â”‚   â””â”€â”€ universal_model_caller.py    # Universal model connector
â”‚   â”‚   â”œâ”€â”€ rca/
â”‚   â”‚   â”‚   â””â”€â”€ feature_attribution.py       # Feature-level RCA
â”‚   â”‚   â”œâ”€â”€ recommendation/
â”‚   â”‚   â”‚   â””â”€â”€ rule_engine.py               # Maps failures to fixes
â”‚   â”‚   â””â”€â”€ storage/
â”‚   â”‚       â””â”€â”€ baseline_store.py            # Baseline storage layer
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ baseline_builder.py              # Baseline service
â”‚       â”œâ”€â”€ investigation_service.py         # Core investigation layer
â”‚       â””â”€â”€ monitoring_service.py            # Monitoring service
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ dashboard.py                         # Streamlit frontend dashboard
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_monitoring.py
â”‚   â”œâ”€â”€ test_detection.py
â”‚   â””â”€â”€ test_recommendation.py
â”‚
â”œâ”€â”€ baselines/                               # Stored model baselines (auto-generated)
â”œâ”€â”€ Dockerfile                               # Docker configuration
â”œâ”€â”€ start.sh                                 # Multi-process startup script
â”œâ”€â”€ requirements.txt                         # Python dependencies
â””â”€â”€ README.md
```

---

## âœ¨ Key Features

### ðŸ“Š Model Monitoring
- Average confidence score tracking
- Confidence standard deviation
- Error rate calculation
- Latency monitoring in milliseconds
- Real-time sample collection counter

### ðŸ“ˆ Baseline Learning
- Automatically learns "normal" model behavior on first run
- Stores baseline metrics per model URL
- Used as reference for all future drift comparisons

### ðŸš¨ Drift & Anomaly Detection
- **KS-Test** â€” statistical confidence distribution drift
- **Variance analysis** â€” confidence explosion detection
- **Feature mean shift** â€” covariate drift per feature
- **Rule-based checks** â€” low confidence, high error rate, silent failures
- **ML-based** â€” Isolation Forest anomaly detection

### ðŸ” Root Cause Analysis (RCA)
- Identifies exactly why failure occurred
- Reports which features contributed most
- Highlights affected user segments
- Assigns severity: `critical` / `high` / `medium` / `low`

### ðŸ› ï¸ Recommendation Engine
- Maps each failure type to specific corrective action
- Actionable suggestions: retrain, rollback, fix pipeline, recalibrate
- Priority-ranked: `CRITICAL` â†’ `HIGH` â†’ `MEDIUM` â†’ `LOW`
- Context-aware â€” different recommendations per failure type

### ðŸ‘¨â€âš–ï¸ Human-in-the-Loop Governance
- Approve / Reject recommendations via dashboard
- No auto-action without explicit user consent
- Full audit log with timestamps
- Compliant with enterprise governance standards

### ðŸ–¥ï¸ Live Dashboard (Streamlit)
- Real-time Plotly charts (confidence, error rate, latency)
- Start / Stop monitoring controls
- CSV data export
- Alerts & anomaly indicators
- Governance approval panel with audit log
- Auto-refreshes every 2 seconds

---

## âš™ï¸ Installation (Local Development)

### 1. Clone the repository
```bash
git clone https://github.com/chaurasiyarishiraj84-alt/Autonomous-ml-failure-investigator.git
cd autonomous-ml-failure-investigator
```

### 2. Create virtual environment
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Mac/Linux
source .venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up environment variables
```bash
cp .env.example .env
```

---

## â–¶ï¸ Running Locally

**Start the backend:**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```
Verify at: `http://localhost:8000/health`

**Start the dashboard:**
```bash
streamlit run ui/dashboard.py
```
Open at: `http://localhost:8501`

---

## ðŸ§ª Quick Test

Create a simple local test model (`test_model.py`):

```python
from fastapi import FastAPI
import random

app = FastAPI()

@app.post("/predict")
def predict(data: dict = None):
    return {
        "prediction": "spam" if random.random() > 0.5 else "not_spam",
        "confidence": round(random.uniform(0.6, 0.95), 3)
    }
```

Run it:
```bash
uvicorn test_model:app --port 9000 --reload
```

In the dashboard enter `http://127.0.0.1:9000/predict` â†’ click **Start Monitoring**.

---

## ðŸ“„ API Reference

### `POST /monitoring/analyze`
Runs full investigation on a model endpoint.

**Request:**
```json
{ "prediction_url": "https://your-model.com/predict" }
```

**Response:**
```json
{
  "metrics": {
    "avg_confidence": 0.85,
    "confidence_std": 0.04,
    "error_rate": 0.02,
    "total_samples": 5
  },
  "drift": {
    "confidence_distribution": {
      "method": "KS-test",
      "drift_detected": false,
      "p_value": 0.43
    }
  },
  "anomalies": [],
  "rca": {
    "root_cause": "none",
    "severity": "none",
    "failure_reason": "Model operating normally"
  },
  "recommendations": [
    { "action": "No action required", "priority": "NONE" }
  ],
  "samples_collected": 5
}
```

### `GET /health`
Returns system health status.

### `POST /monitoring/prediction`
Ingest a single prediction log manually.

---

## ðŸ“š Tech Stack

| Layer | Technology |
|---|---|
| Backend API | FastAPI + Uvicorn |
| Frontend Dashboard | Streamlit |
| Data Validation | Pydantic |
| Drift Detection | SciPy (KS-test), NumPy |
| Anomaly Detection | scikit-learn (Isolation Forest) |
| Explainability | SHAP |
| Visualization | Plotly |
| Storage | JSON-based baseline store |
| Logging | Python logging + Loguru |
| Containerization | Docker |

---

## ðŸ” Security Design

- **Read-only access** â€” system never modifies user models or data
- **No source code access** â€” monitors behavior via API only
- **No data ownership** â€” raw data never stored long-term
- **Permission-based actions** â€” all critical actions require explicit user consent
- **Audit trail** â€” every approval/rejection logged with timestamp
- **PII-safe** â€” only aggregated metrics stored, never raw features

---

## ðŸ› Troubleshooting

| Problem | Error | Solution |
|---|---|---|
| Backend not starting | `Address already in use` | Kill process on port 8000 |
| Dashboard not loading | `ConnectionError` | Start backend first on port 8000 |
| Module not found | `No module named 'fastapi'` | Activate venv â†’ `pip install -r requirements.txt` |
| Samples stuck at 0 | â€” | Ensure model returns `confidence` field |
| autorefresh not found | â€” | `pip install streamlit-autorefresh` |
| pydantic error | `No module named 'pydantic_settings'` | `pip install pydantic-settings` |
| Confidence always 0.0 | â€” | Use a public URL, not localhost on HuggingFace |

---

## ðŸŒ Deployment

- âœ… Docker ready
- âœ… HuggingFace Spaces compatible
- âœ… Cloud-ready (AWS, GCP, Azure)
- âœ… CI/CD compatible

```bash
docker build -t ml-failure-investigator .
docker run -p 7860:7860 ml-failure-investigator
```

---

## ðŸŽ¯ Use Cases

| Domain | Example |
|---|---|
| Finance | Fraud model drift detection |
| Healthcare | Silent diagnostic model failure prevention |
| E-commerce | Recommendation quality degradation |
| Autonomous Driving | Edge model anomaly analysis |
| AI SaaS | Customer model health monitoring |

---

## ðŸ† Tier-1+ MLOps Compliance

| Feature | Status |
|---|---|
| Continuous monitoring | âœ… |
| Explainable AI (XAI) | âœ… |
| Automated root-cause analysis | âœ… |
| Human-in-the-loop governance | âœ… |
| Audit logging & compliance | âœ… |
| Domain-agnostic core | âœ… |
| Safe advisory-first design | âœ… |
| Statistical drift detection | âœ… |
| Auto-refresh live dashboard | âœ… |

---

## ðŸ‘¤ Author

**Rishi Raj Chaurasiya**
B.Tech â€” Artificial Intelligence & Machine Learning

---

## ðŸ“œ License

MIT License â€” Open Source

---

> *This project demonstrates how real production ML systems are monitored, debugged, and governed â€” not just how models are trained. It reflects industry-standard MLOps thinking used at scale.*
