---
title: Autonomous ML Failure Investigator
emoji: üß†
colorFrom: purple
colorTo: blue
sdk: streamlit
app_file: ui/dashboard.py
pinned: true
license: mit
---

# üß† Autonomous ML Failure Investigator

> A Tier-1+ production-grade MLOps system that continuously monitors deployed ML models, detects failures, explains root causes, and recommends corrective actions ‚Äî autonomously and intelligently.

> üí° **Running on HuggingFace Spaces?** No setup needed ‚Äî the app is already live above. The installation and localhost instructions below are for **local development only.**

**Autonomous ML Failure Investigator** is an advanced Model Observability, Drift Detection, and Root Cause Analysis (RCA) system designed to monitor deployed machine learning models in real time, detect failures, explain why they occur, and provide actionable recommendations with human-in-the-loop governance.

This project simulates industry-grade MLOps monitoring platforms used in production environments by companies like Google, Netflix, and Amazon.

---

## ü©∫ The Core Idea

Think of this system as a **doctor for ML models in production.**

| Doctor | This System |
|---|---|
| Patient | Deployed ML model |
| Symptoms | Accuracy drop, drift, anomalies |
| Tests | Metrics, KS-test, SHAP |
| Diagnosis | Root Cause Analysis |
| Explanation | Human-readable report |
| Medicine | Retrain, rollback, fix pipeline |
| Surgery (optional) | Auto-fix with human approval |
| Medical history | Failure memory & learning |

---

## üöÄ What This Project Does

For any deployed ML model, this system:

- **Continuously monitors** prediction behavior via API
- **Learns a baseline** of normal model performance automatically
- **Detects drift and anomalies** using statistical tests and ML
- **Explains why failures happen** through Root Cause Analysis
- **Identifies which features** are responsible for failures
- **Generates actionable recommendations** specific to each failure type
- **Requires human approval** before any critical action is taken
- **Produces a diagnostic report** for model improvement
- **Learns from past failures** to improve detection over time

---

## üèóÔ∏è System Architecture

```
User connects model via API URL
         ‚Üì
Data Capture Layer          ‚Üí logs inputs, predictions, latency
         ‚Üì
Baseline Learning           ‚Üí learns normal behavior automatically
         ‚Üì
Drift & Anomaly Detection   ‚Üí KS-test, variance, feature shift
         ‚Üì
Root Cause Analysis (RCA)   ‚Üí why did it fail? which feature?
         ‚Üì
Explanation Layer           ‚Üí plain English findings
         ‚Üì
Recommendation Engine       ‚Üí specific actionable fixes
         ‚Üì
Human Approval (Governance) ‚Üí approve/reject with audit log
         ‚Üì
Continuous Learning         ‚Üí improves from past failures
```

---

## üìÇ Project Structure

```
autonomous-ml-failure-investigator/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                              # FastAPI entry point
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ auth.py                      # Authentication routes
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ model.py                     # Model registration routes
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ monitoring.py                # Monitoring API endpoints
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ recommendation.py            # Recommendation routes
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ automation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decision_engine.py           # Auto-action decision logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deploy_manager.py            # Safe model deployment
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor.py                  # Action executor
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_validator.py           # Model validation checks
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retrain_pipeline.py          # Retraining trigger
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baselines/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_builder.py          # Baseline construction
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ baseline_store.py            # Baseline persistence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detection/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accuracy_drift_detector.py   # Accuracy drift detection
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomaly_detector.py          # Rule + ML anomaly detection
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ drift_detector.py            # KS-test, variance, feature drift
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metric_checker.py            # Threshold-based metric checks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ governance/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ approval_flow.py             # Human approval + audit logs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learning/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ failure_memory.py            # Stores past failure patterns
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pattern_learner.py           # Learns from failure history
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ baseline_builder.py          # Metrics computation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observer/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accuracy_tracker.py          # Tracks accuracy over time
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_collector.py            # Collects prediction data
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_prober.py              # Probes model behavior
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_watcher.py             # Continuous model watcher
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ probing/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_prober.py              # Model probe logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ payload_generator.py         # Test payload generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ universal_model_caller.py    # Universal model connector
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rca/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_attribution.py       # Feature-level RCA
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ root_cause_analyzer.py       # Main RCA engine
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shap_explainer.py            # SHAP-based explainability
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommendation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommender.py               # Recommendation orchestrator
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rule_engine.py               # Maps failures to fixes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ baseline_store.py            # Baseline storage layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ serialization.py             # JSON/data serialization
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                        # Database models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session.py                       # DB session management
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py                          # Auth request/response models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py                         # Model schema
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring.py                    # Monitoring schema
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recommendation.py               # Recommendation schema
‚îÇ   ‚îú‚îÄ‚îÄ security/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py                          # Auth & token handling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ encryption.py                    # Data encryption
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth_service.py                  # Authentication service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ automation_service.py            # Auto-action service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_builder.py              # Baseline service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detection_service.py             # Detection orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ investigation_service.py         # Core investigation layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring_service.py            # Monitoring service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring_orchestrator.py       # Monitoring orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recommendation_service.py        # Recommendation service
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ config.py                        # Settings & environment
‚îÇ       ‚îî‚îÄ‚îÄ logger.py                        # Structured logging
‚îÇ
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py                         # Streamlit frontend dashboard
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_monitoring.py                   # Monitoring tests
‚îÇ   ‚îú‚îÄ‚îÄ test_detection.py                    # Detection tests
‚îÇ   ‚îî‚îÄ‚îÄ test_recommendation.py              # Recommendation tests
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ audit_logs/                          # Governance audit trail
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ approvals.json                   # Human approval log
‚îÇ   ‚îî‚îÄ‚îÄ samples/                             # Safe example data
‚îÇ
‚îú‚îÄ‚îÄ baselines/                               # Stored model baselines (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md                      # System architecture details
‚îÇ   ‚îú‚îÄ‚îÄ threat_model.md                      # Security threat model
‚îÇ   ‚îî‚îÄ‚îÄ user_guide.md                        # User guide
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile                           # Docker configuration
‚îÇ
‚îú‚îÄ‚îÄ dummy_model/                             # Test model for local testing
‚îÇ
‚îú‚îÄ‚îÄ .env.example                             # Environment variables template
‚îú‚îÄ‚îÄ .gitignore                               # Git ignore rules
‚îú‚îÄ‚îÄ Dockerfile                               # Root Dockerfile
‚îú‚îÄ‚îÄ requirements.txt                         # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## ‚ú® Key Features

### üìä Model Monitoring
- Average confidence score
- Confidence standard deviation
- Error rate tracking
- Latency monitoring
- Real-time sample collection counter

### üìà Baseline Learning
- Automatically learns "normal" model behavior on first run
- Stores baseline metrics per model URL
- Used as reference for all future drift comparisons

### üö® Drift & Anomaly Detection
- **KS-Test** ‚Äî statistical confidence distribution drift
- **Variance analysis** ‚Äî confidence explosion detection
- **Feature mean shift** ‚Äî covariate drift per feature
- **Rule-based checks** ‚Äî low confidence, high error rate, silent failures
- **ML-based** ‚Äî Isolation Forest anomaly detection

### üîç Root Cause Analysis (RCA)
- Identifies exactly why failure occurred
- Reports which features contributed most
- Highlights affected user segments
- Assigns severity: critical / high / medium / low

### üß† Explainability
- Feature importance analysis via SHAP
- Identifies drifting features
- Plain English failure explanations
- Business-safe, audit-ready reports

### üõ†Ô∏è Recommendation Engine
- Maps each failure type to specific corrective action
- Actionable suggestions: retrain, rollback, fix pipeline, recalibrate
- Priority-ranked: CRITICAL ‚Üí HIGH ‚Üí MEDIUM ‚Üí LOW
- Context-aware ‚Äî different recommendations per failure type

### üë®‚Äç‚öñÔ∏è Human-in-the-Loop Governance
- Approve / Reject recommendations via dashboard
- No auto-action without explicit user consent
- Full audit log with timestamps
- Compliant with enterprise governance standards

### üñ•Ô∏è Dashboard (Streamlit)
- Real-time Plotly charts (confidence, error rate, latency)
- Start / Stop monitoring controls
- CSV data export
- Alerts & anomaly indicators
- Governance approval buttons with audit log
- Auto-refreshes every 2 seconds

---

## ‚öôÔ∏è Installation (Local Development)

### 1. Clone the repository
```bash
git clone https://github.com/chaurasiyarishiraj84-alt/Autonomous-ml-failure-investigator.git
cd autonomous-ml-failure-investigator
```

### 2. Create virtual environment
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Mac/Linux
source .venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up environment variables
```bash
cp .env.example .env
```

---

## ‚ñ∂Ô∏è Running Locally

### Start the backend
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Verify it's running:
```
http://localhost:8000/health
```

### Start the dashboard
```bash
streamlit run ui/dashboard.py
```

Open in browser:
```
http://localhost:8501
```

---

## üß™ Quick Test

Create a simple test model (`test_model.py`):

```python
from fastapi import FastAPI
import random

app = FastAPI()

@app.post("/predict")
def predict(data: dict):
    return {
        "prediction": "spam" if random.random() > 0.5 else "not_spam",
        "confidence": round(random.uniform(0.6, 0.95), 3)
    }
```

Run it:
```bash
uvicorn test_model:app --port 9000 --reload
```

In the dashboard, enter `http://127.0.0.1:9000/predict` and click **Start Monitoring**.

To simulate failures and test RCA & recommendations:
```python
"confidence": round(random.uniform(0.1, 0.3), 3)  # triggers low confidence alert
```

---

## üìÑ API Reference

### `POST /monitoring/analyze`

Runs full investigation on a model endpoint.

**Request:**
```json
{
  "prediction_url": "http://your-model.com/predict"
}
```

**Response:**
```json
{
  "metrics": {
    "avg_confidence": 0.85,
    "confidence_std": 0.04,
    "error_rate": 0.0,
    "total_samples": 5
  },
  "drift": {
    "confidence_distribution": {
      "method": "KS-test",
      "statistic": 0.2,
      "p_value": 0.43,
      "drift_detected": false
    }
  },
  "anomalies": [],
  "rca": {
    "root_cause": "none",
    "severity": "none",
    "failure_reason": "Model operating normally"
  },
  "recommendations": [
    { "action": "No action required", "priority": "NONE" }
  ],
  "samples_collected": 5
}
```

### `GET /health`
Returns system health status.

### `POST /monitoring/prediction`
Ingest a single prediction log manually.

---

## üìö Tech Stack

| Layer | Technology |
|---|---|
| Backend API | FastAPI + Uvicorn |
| Frontend Dashboard | Streamlit |
| Data Validation | Pydantic |
| Drift Detection | SciPy (KS-test), NumPy |
| Anomaly Detection | scikit-learn (Isolation Forest) |
| Explainability | SHAP |
| Visualization | Plotly |
| Storage | JSON-based baseline store |
| Logging | Python logging + Loguru |
| Containerization | Docker |

---

## üîê Security Design

- **Read-only access** ‚Äî system never modifies user models or data
- **No source code access** ‚Äî monitors behavior via API only
- **No data ownership** ‚Äî raw data never stored long-term
- **Permission-based actions** ‚Äî all critical actions require explicit user consent
- **Audit trail** ‚Äî every approval/rejection logged with timestamp
- **PII-safe** ‚Äî only aggregated metrics stored, never raw features

---

## üîå Supported Model Types

| Type | Example |
|---|---|
| Generic REST API | Any `POST /predict` endpoint |
| HuggingFace Inference API | `https://api-inference.huggingface.co/...` |
| Gradio Apps | `/run/predict` endpoints |
| Local models | `http://localhost:PORT/predict` |

---

## üêõ Troubleshooting

### Problem: Backend not starting
**Error:** `ERROR: Address already in use`

**Solution:**
```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Mac/Linux
lsof -i :8000
kill -9 <PID>
```

---

### Problem: Streamlit dashboard not loading
**Error:** `ConnectionError: Failed to connect to backend`

**Solution:** Make sure backend is running first on port 8000, then start dashboard:
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

---

### Problem: ModuleNotFoundError
**Error:** `ModuleNotFoundError: No module named 'fastapi'`

**Solution:** Activate virtual environment first, then:
```bash
pip install -r requirements.txt
```

---

### Problem: RCA variable error
**Error:** `UnboundLocalError: cannot access local variable 'rca'`

**Solution:** Remove this line from `investigation_service.py`:
```python
from app.core import rca  # delete this line
```

---

### Problem: Samples Collected stays at 0
**Solution:** Make sure your model returns a `confidence` field in its response:
```json
{ "prediction": "spam", "confidence": 0.87 }
```

---

### Problem: streamlit-autorefresh not found
**Solution:**
```bash
pip install streamlit-autorefresh
```

---

### Problem: SHAP or Evidently install fails
**Solution:**
```bash
pip install shap --no-cache-dir
pip install evidently --no-cache-dir
```

---

### Problem: pydantic_settings import error
**Error:** `ModuleNotFoundError: No module named 'pydantic_settings'`

**Solution:**
```bash
pip install pydantic-settings
```

---

### Problem: Port already in use for test model
**Solution:** Run test model on a different port:
```bash
uvicorn test_model:app --port 9001 --reload
# Then use http://127.0.0.1:9001/predict in dashboard
```

---

## üß™ Testing

```bash
pytest tests/
```

---

## üåç Deployment

- ‚úÖ Docker ready
- ‚úÖ Hugging Face Spaces compatible
- ‚úÖ Cloud-ready architecture (AWS, GCP, Azure)
- ‚úÖ CI/CD compatible

```bash
docker build -t ml-failure-investigator .
docker run -p 8000:8000 ml-failure-investigator
```

---

## üéØ Use Cases

| Domain | Example |
|---|---|
| Finance | Fraud model drift detection |
| Healthcare | Silent diagnostic model failure prevention |
| E-commerce | Recommendation quality degradation |
| Autonomous Driving | Edge model anomaly analysis |
| AI SaaS | Customer model health monitoring |

---

## üèÜ Tier-1+ MLOps Compliance

| Feature | Status |
|---|---|
| Continuous monitoring | ‚úÖ |
| Explainable AI (XAI) | ‚úÖ |
| Automated root-cause analysis | ‚úÖ |
| Human-in-the-loop governance | ‚úÖ |
| Audit logging & compliance | ‚úÖ |
| Domain-agnostic core | ‚úÖ |
| Safe advisory-first design | ‚úÖ |
| Statistical drift detection | ‚úÖ |
| Auto-refresh live dashboard | ‚úÖ |

---

## üë§ Author

**Rishi Raj Chaurasiya**
B.Tech ‚Äî Artificial Intelligence & Machine Learning

---

## üìú License

MIT License ‚Äî Open Source

---

> This project demonstrates how real production ML systems are monitored, debugged, and governed ‚Äî not just how models are trained. It reflects industry-standard MLOps thinking used at scale.